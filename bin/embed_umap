#!/usr/bin/env python
# -*- tab-width:4;indent-tabs-mode:nil;show-trailing-whitespace:t;rm-trailing-spaces:t -*-
# vi: set ts=2 noet:

import sys
import shutil
import argparse

import pandas as pd
import pyarrow.parquet
import pyarrow as pa
import joblib
import numpy as np
from MPLearn import embedding
from MPLearn import embedding_notebook
import hdbscan



DESCRIPTION = """Do the embedding for one dataset with one set of parameters
version 0.0.1

Example:
Fit and embed the full cell features matrix:

    cd MPLearn/vignettes/Steatosis2020/umap_embedding_200217
    embed_umap \
        --dataset intermediate_data/cf10k.parquet \
        --tag cell_features_pca_umap2_15_0.0 \
        --umap_n_components 2 \
        --umap_n_neighbors 15 \
        --umap_min_dist 0.0

Embed a 10k subset of cell features given a reference embedding:

    cd MPLearn/vignettes/Steatosis2020/umap_embedding_200217
    python scripts/umap_embedding.py \
        --dataset intermediate_data/cf10k.parquet \
        --tag cf10k_pca_umap2_15_0.0_euclid \
        --ref_embed_dir intermediate_data/cell_features_pca_umap2_15_0.0

"""

def main(argv):
    parser = argparse.ArgumentParser(description=DESCRIPTION)
    parser.add_argument(
        "--dataset", nargs="+", type=str, action="store", dest="dataset",
        help="""Path to a parquet table on disk (Required)""")
    parser.add_argument(
        "--feature_columns", type=str, action="store", dest="feature_columns", default=None,
        help="""A tab serparated file with a column 'feature' containing cell feature columns to read in (Default: use all columns)""")
    parser.add_argument(
        "--random_subset", type=int, action="store", dest="random_subset", default=None,
        help="""Filter to a random subset of rows of the given size (Default: use all rows)""")
    parser.add_argument(
        "--tag", type=str, action="store", dest="tag",
        help="""Identifier for embedding (Required)""")
    parser.add_argument(
        "--ref_embed_dir", type=str, action="store", dest="ref_embed_dir", default=None,
        help="""Directory of previously computed embedding to use to re-embed the given dataset (Default: Do not re-embed)""")
    parser.add_argument(
        "--re_embed_batch_size", type=int, action="store", dest="re_embed_batch_size", default=100000,
        help="""When re-embedding, batch the inputs in to subsets of this size (Default: re-embed the whole dataset in a single batch)""")
    parser.add_argument(
        "--standardize_features", action="store_true", dest="standardize_features", default=None,
        help="""Standardize each feature to have mean 0 and unit variance (Default: standardize when building an embedding, follow what was done when re-embedding)""")
    parser.add_argument(
        "--no_standardize_features", action="store_false", dest="standardize_features", default=None,
        help="""Standardize each feature to have mean 0 and unit variance (Default: standardize when building an embedding, follow what was done when re-embedding)""")
    parser.add_argument(
        "--pca_n_components", type=int, action="store", dest="pca_n_components", default=None,
        help="""Dimension of the preliminary PCA embedding (Default: PCA with no-dimensionality reduction)""")
    parser.add_argument(
        "--pca_batch_size", type=int, action="store", dest="pca_batch_size", default=1000,
        help="""Batch size of streaming PCA embedding (Default: min(n_features, 1000))""")
    parser.add_argument(
        "--umap_n_components", type=int, action="store", dest="umap_n_components", default=2,
        help="""Dimension of the resulting UMAP embedding (Default: 2)""")
    parser.add_argument(
        "--umap_n_neighbors", type=int, action="store", dest="umap_n_neighbors", default=15,
        help="""Number of neighbors when computing UMAP embedding (Default: 15)""")
    parser.add_argument(
        "--umap_min_dist", type=float, action="store", dest="umap_min_dist", default=0.0,
        help="""Minimum distance between points in UMAP embedding (Default: 0.0)""")
    parser.add_argument(
        "--umap_a", type=float, action="store", dest="umap_a", default=None,
        help="""UMAP parameter a (Default: None)""")
    parser.add_argument(
        "--umap_b", type=float, action="store", dest="umap_b", default=None,
        help="""UMAP parameter b (Default: None)""")
    parser.add_argument(
        "--umap_negative_sample_rate", type=int, action="store", dest="umap_negative_sample_rate", default=5,
        help="""UMAP negative sample rate (Default: 5x)""")
    parser.add_argument(
        "--umap_metric", type=str, action="store", dest="umap_metric", default='cosine',
        help="""UMAP Metric. (Default: cosine) """)
    parser.add_argument(
        "--umap_init", type=str, action="store", dest="umap_init", default='spectral',
        help="""UMAP initialization. Spectral is preferred, but random is more robust (Default: spectral)""")
    parser.add_argument(
        "--umap_low_memory", action="store_true", dest="umap_low_memory", default=False,
        help="""Use UMAP's low-memory NN-lokup, may be slower (Default: False)""")
    parser.add_argument(
        "--umap_n_epochs", type=int, action="store", dest="umap_n_epochs", default=None,
        help="""UMAP number of epochs (Default: None)""")
    parser.add_argument(
        "--no_save_transform", action="store_false", dest="save_transform", default=True,
        help="""Save the computed embedding transformation to re-embed other points (Default: True)""")
    parser.add_argument(
        "--hdbscan_min_cluster_size", type=int, action="store", dest="hdbscan_min_cluster_size", default=100,
        help="""HDBSCAN minimum cluster size (Default: 100)""")
    parser.add_argument(
        "--no_compute_hdbscan_clusters", action="store_false", dest="compute_hdbscan_clusters", default=True,
        help="""Compute HDBSCAN clusters and store in the tagged directory (Default: True)""")
    parser.add_argument(
        "--no_save_clusterer", action="store_false", dest="save_clusterer", default=True,
        help="""Save the clusterer to assign cluster labels to new points (Default: True)""")
    parser.add_argument(
        "--seed", type=int, action="store", dest="seed", default=14730219,
        help="""Initialize random state with this seed. Default is Nicolaus Copernicus' birthday""")
    parser.add_argument(
        "--verbose", action="store_true", dest="verbose", default=False,
        help="""Verbose output""")

    arguments = parser.parse_args()

    #check for incompatible flags
    if arguments.ref_embed_dir is not None:
        if arguments.pca_n_components is not None:
            print("ERROR: setting '--pca_n_components' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.pca_batch_size != parser.get_default('pca_batch_size'):
            print("ERROR: setting '--pca_batch_size' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_n_components != parser.get_default('umap_n_components'):
            print("ERROR: setting '--umap_n_components' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_n_neighbors != parser.get_default('umap_n_neighbors'):
            print("ERROR: setting '--umap_n_neighbors' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_min_dist != parser.get_default('umap_min_dist'):
            print("ERROR: setting '--umap_min_dist' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_a != parser.get_default('umap_a'):
            print("ERROR: setting '--umap_a' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_b != parser.get_default('umap_b'):
            print("ERROR: setting '--umap_b' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_negative_sample_rate != parser.get_default('umap_negative_sample_rate'):
            print("ERROR: setting '--umap_negative_sample_rate' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_metric != parser.get_default('umap_metric'):
            print("ERROR: setting '--umap_metric' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_init != parser.get_default('umap_init'):
            print("ERROR: setting '--umap_init' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.umap_low_memory != parser.get_default('umap_low_memory'):
            print("ERROR: setting '--umap_low_memory' is incompatible with setting '--ref_embed_dir'")
            exit(1)
        if arguments.save_transform != parser.get_default('save_transform'):
            print("ERROR: setting '--no_save_transform' is incompatible with setting '--ref_embed_dir'")
            exit(1)
    else:
        if arguments.re_embed_batch_size != parser.get_default('re_embed_batch_size'):
            print("ERROR: setting '--re_embed_batch_size' requires setting '--ref_embed_dir'")
            exit(1)

    random_state = np.random.RandomState(seed=arguments.seed)

    ################
    # Load dataset #
    ################
    if arguments.feature_columns is not None:
        feature_columns = pd.read_csv(arguments.feature_columns, sep="\t")
        if arguments.verbose:
            print("Loading '{}' feature columns".format(feature_columns.shape[0]))
    else:
        feature_columns = None

    dataset = []
    for dataset_path in arguments.dataset:
        if arguments.verbose:
            print("Reading in dataset '{}'...".format(dataset_path))
        if arguments.feature_columns is not None:
            dataset.append(pa.parquet.read_table(
                source=dataset_path,
                columns=feature_columns['feature'].to_list(),
            ).to_pandas().astype('float32'))
        else:
            dataset.append(pa.parquet.read_table(
                source=dataset_path,
            ).to_pandas().astype('float32'))
    dataset = pd.concat(dataset, axis=0)
    if arguments.feature_columns is not None:
        if arguments.verbose:
            print("Transforming individual features ...")
        for i in range(feature_columns.shape[0]):
            feature = feature_columns['feature'][i]
            transform = feature_columns['transform'][i]
            if transform == 'identity':
                pass
            elif transform == 'log':
                dataset[feature] = np.log(dataset[feature])
            elif transform == 'log1p':
                dataset[feature] = np.log1p(dataset[feature])
            else:
                raise Exception("Unrecognized tranform '{}' for feature column '{}' in passed argument --feature_columns '{}'".format(transform, feature, arguments.feature_columns))

    if arguments.random_subset is not None and arguments.random_subset < dataset.shape[0]:
        if arguments.verbose:
            print("Using a random subset of '{}' of the total '{}' rows.".format(arguments.random_subset, dataset.shape[0]))
        sample_indices = np.random.choice(dataset.shape[0], arguments.random_subset, replace=False)
        dataset = dataset.iloc[sample_indices]
    else:
        if arguments.verbose:
            print("Using all '{}' rows.".format(dataset.shape[0]))

    #####################
    # Compute Embedding #
    #####################
    if arguments.ref_embed_dir is not None:
        if arguments.verbose:
            print("Emebedding dataset in reference UMAP {} ...".format(arguments.ref_embed_dir))
        embed_dir = "intermediate_data/{}".format(arguments.tag)
        umap_embedding = embedding.embed(
            dataset=dataset,
            embed_dir=embed_dir,
            standardize_features=arguments.standardize_features,
            ref_embed_dir="{}".format(arguments.ref_embed_dir),
            batch_size=arguments.re_embed_batch_size,
            n_epochs=arguments.umap_n_epochs,
            verbose=arguments.verbose)

        clusterer_fname = "{}/hdbscan_cluster_min{}.joblib".format(
            arguments.ref_embed_dir, arguments.hdbscan_min_cluster_size)
        try:
            clusterer = joblib.load(clusterer_fname)
        except:
            if arguments.verbose:
                print((
                    f"HDBSCAN Clusterer not found at {clusterer_fname}, "
                    f"assume no-clusterer was created"))
            clusterer = None
        if clusterer:
            cluster_labels = hdbscan.approximate_predict(clusterer, umap_embedding)
            cluster_labels = pd.DataFrame(cluster_labels, columns=['cluster_label'])
            pa.parquet.write_table(
                table=pa.Table.from_pandas(cluster_labels),
                where=("intermediate_data/{}/hdbscan_clustering_min{}.parquet".format(arguments.tag, arguments.hdbscan_min_cluster_size)))

    else:
        if arguments.verbose:
            print("Computing UMAP embedding ...")

        embed_dir = "intermediate_data/{}".format(arguments.tag)
        umap_embedding = embedding.fit_embedding(
            dataset=dataset,
            embed_dir=embed_dir,
            standardize_features=arguments.standardize_features is None or arguments.standardize_features,
            pca_n_components=arguments.pca_n_components,
            pca_batch_size=min(arguments.pca_batch_size, dataset.shape[1]),
            umap_n_components=arguments.umap_n_components,
            umap_init=arguments.umap_init,
            umap_n_neighbors=arguments.umap_n_neighbors,
            umap_min_dist=arguments.umap_min_dist,
            umap_a=arguments.umap_a,
            umap_b=arguments.umap_b,
            umap_negative_sample_rate=arguments.umap_negative_sample_rate,
            umap_metric=arguments.umap_metric,
            umap_n_epochs=arguments.umap_n_epochs,
            low_memory=arguments.umap_low_memory,
            save_transform=arguments.save_transform,
            seed=arguments.seed,
            verbose=arguments.verbose)


        if arguments.compute_hdbscan_clusters:
            if arguments.verbose:
                print("Computing HDBSCAN clusters ...")
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=arguments.hdbscan_min_cluster_size,
                prediction_data=True)
            cluster_labels = clusterer.fit_predict(umap_embedding)
            cluster_labels = pd.DataFrame(cluster_labels, columns=['cluster_label'])
            if arguments.save_clusterer:
                clusterer_path = "intermediate_data/{}/hdbscan_clusterer_min{}.joblib".format(
                        arguments.tag, arguments.hdbscan_min_cluster_size)
                if arguments.verbose:
                    print("Saving HDBSCAN clusterer to {} ...".format(clusterer_path))
                joblib.dump(value=clusterer, filename=clusterer_path)
            pa.parquet.write_table(
                table=pa.Table.from_pandas(cluster_labels),
                where="intermediate_data/{}/hdbscan_clustering_min{}.parquet".format(arguments.tag, arguments.hdbscan_min_cluster_size))

    if arguments.feature_columns is not None:
        shutil.copy(arguments.feature_columns, f"{embed_dir}/feature_columns.tsv")

    if arguments.random_subset is not None and arguments.random_subset < dataset.shape[0]:
        with open("{}/sample_indices.tsv".format(embed_dir), 'w') as f:
            f.write("\n".join(map(str, sample_indices)))

    embedding_notebook.save_embedding_plot(
        embedding=umap_embedding,
        output_fname=f"{embed_dir}/embedding.png")



if __name__ == "__main__":
    main(sys.argv)
